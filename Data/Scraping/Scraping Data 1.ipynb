{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data from a transport and logistics website 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps : \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1- Get companies links\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2- Get data using those links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tools and libraries\n",
    "\n",
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of cities that we will be targeting\n",
    "Ville1 = [\"City1\",\"City2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to get links:\n",
    "# Think of changing your Ip Adress to not get banned from the website\n",
    "\n",
    "\n",
    "lines = []\n",
    "\n",
    "browser = wd.Firefox(executable_path=r'C:\\Users\\USER\\geckodriver.exe')\n",
    "browser.get(\"https://www.website1.com\")\n",
    "\n",
    "try:\n",
    "    accept = WebDriverWait(browser,10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"#didomi-notice-agree-button\")))\n",
    "    accept.click()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "\n",
    "for ville in Ville1:\n",
    "        try:\n",
    "            quoiqui = WebDriverWait(browser,10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"input[name='quoiqui']\")))\n",
    "            quoiqui.clear()\n",
    "            quoiqui.send_keys(\"transport\")\n",
    "            ou = WebDriverWait(browser,10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"input[name='ou']\")))\n",
    "            ou.clear() \n",
    "            ou.send_keys(ville)\n",
    "            \n",
    "            trouver = WebDriverWait(browser,10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"button[title='Trouver']\")))\n",
    "            trouver.click()\n",
    "            \n",
    "            number = browser.find_element_by_css_selector('#SEL-compteur')\n",
    "            j = number.text[9:]\n",
    "            print(int(j))\n",
    "            i=1\n",
    "            \n",
    "            while i<= int(j):\n",
    "                try:       \n",
    "                    links = browser.find_elements_by_class_name(\"denomination-links\")\n",
    "                    for link in links:\n",
    "                        lines.append(link.get_attribute('href'))\n",
    "        \n",
    "                    pagination = browser.find_element_by_id(\"pagination-next\")\n",
    "                    pagination.click()\n",
    "                    i=i+1\n",
    "                except Exception as e2:\n",
    "                    print(e2)\n",
    "                    \n",
    "                \n",
    "        \n",
    "        except Exception as e1:\n",
    "            print(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store links in a file (text, csv, excel...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data from links scraped\n",
    "\n",
    "\n",
    "\n",
    "j=0\n",
    "ProxyHttps = [\n",
    "#Put a list of free/premium Ip Adresses\n",
    "]\n",
    "\n",
    "\n",
    "Nom = []\n",
    "Logo = []\n",
    "Secteur = []\n",
    "Description = []\n",
    "Activite = []\n",
    "Adresse = []\n",
    "Tel = []\n",
    "\n",
    "i=0;\n",
    "while i < 598: # lines lenght is 598\n",
    "    \n",
    "    browser.get(lines[i])\n",
    "    try:\n",
    "        accept = WebDriverWait(browser,10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"#didomi-notice-agree-button\")))\n",
    "        accept.click()\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        title = soup.find('h1', {'class': 'noTrad'})\n",
    "        Nom.append(title.get_text().strip())\n",
    "    except Exception as e:\n",
    "        Nom.append(\" \")\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        div = soup.find('div', {'class':'teaser-rub'})\n",
    "        Secteur.append(div.get_text())\n",
    "    except Exception as e:\n",
    "        Secteur.append(\" \")\n",
    "\n",
    "    \n",
    "    adr = \"\"\n",
    "    try:\n",
    "        adresse = soup.find('a', {'class':'teaser-item'})\n",
    "        for span in adresse:\n",
    "            adr = adr + span.text\n",
    "        Adresse.append(adr[13:])\n",
    "    except Exception as e:\n",
    "        Adresse.append(\" \")\n",
    "    \n",
    "    try:\n",
    "        tel = soup.find('span', {'class':'coord-numero noTrad'})\n",
    "        Tel.append(tel.text)\n",
    "    except Exception as e:\n",
    "        Tel.append(\" \")\n",
    "        \n",
    "    try:\n",
    "        images = soup.find_all('img')\n",
    "        for pic in images:\n",
    "            if pic['src'].find(\"https://www.website1/media\") != -1: # url image contains webiste name\n",
    "                break;\n",
    "        Logo.append(pic['src'])\n",
    "    except Exception as e:\n",
    "        Logo.append(\" \")\n",
    "        \n",
    "    \n",
    "    try : \n",
    "        prestations = soup.find('ul', {'class':'list-v'})\n",
    "        Description.append(prestations.text.rstrip().replace(\"\\n\",\" , \"))\n",
    "    except Exception as e:\n",
    "        Description.append(\" \")\n",
    "        \n",
    "    \n",
    "    try : \n",
    "        activite = soup.find('ul', {'class':'clearfix list-l'})\n",
    "        Activite.append(activite.text.rstrip().replace(\"\\n\",\" \"))\n",
    "    except Exception as e:\n",
    "        Activite.append(\" \")\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    if i%20 == 0:\n",
    "\n",
    "        wd.DesiredCapabilities.FIREFOX[ProxyHttps[j]] = {\n",
    "              \"httpProxy\":ProxyHttps[j],\n",
    "              \"ftpProxy\":ProxyHttps[j],\n",
    "              \"sslProxy\":ProxyHttps[j],\n",
    "              \"noProxy\":None,\n",
    "              \"proxyType\":\"MANUAL\",\n",
    "              \"class\":\"org.openqa.selenium.Proxy\",\n",
    "              \"autodetect\":False\n",
    "              }\n",
    "        j=j+1\n",
    "    \n",
    "    i=i+1    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Data in a file (text, csv, excel...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
